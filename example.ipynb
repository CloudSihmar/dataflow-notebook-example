{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d874f366-922a-4784-85bd-8a26f5b63cde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DONE'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "# Initialize the pipeline with the DirectRunner\n",
    "p = beam.Pipeline()\n",
    "\n",
    "# Here you can define your pipeline steps (currently, it does nothing)\n",
    "\n",
    "# Run the pipeline\n",
    "result = p.run()\n",
    "\n",
    "# Wait until the pipeline execution is complete\n",
    "result.wait_until_finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f45312f8-3405-4396-9e97-c412023d0f17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "    # This is a placeholder for pipeline operations\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a05316-5abd-4950-917d-76173ae3e0f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Apache', 1)\n",
      "('Beam', 1)\n",
      "('is', 2)\n",
      "('a', 2)\n",
      "('unified', 1)\n",
      "('programming', 1)\n",
      "('model', 2)\n",
      "('for', 1)\n",
      "('both', 1)\n",
      "('batch', 1)\n",
      "('and', 2)\n",
      "('streaming', 1)\n",
      "('data', 2)\n",
      "('processing.', 1)\n",
      "('It', 1)\n",
      "('provides', 1)\n",
      "('way', 1)\n",
      "('to', 2)\n",
      "('write', 1)\n",
      "('processing', 1)\n",
      "('pipelines', 1)\n",
      "('run', 1)\n",
      "('them', 1)\n",
      "('on', 1)\n",
      "('any', 1)\n",
      "('execution', 1)\n",
      "('engine.', 1)\n",
      "(\"Beam's\", 1)\n",
      "('easy', 1)\n",
      "('use', 1)\n",
      "('yet', 1)\n",
      "('powerful', 1)\n",
      "('in', 1)\n",
      "('its', 1)\n",
      "('capabilities.', 1)\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "def run_pipeline():\n",
    "    # Sample texts\n",
    "    texts = [\n",
    "        'Apache Beam is a unified programming model for both batch and streaming data processing.',\n",
    "        'It provides a way to write data processing pipelines and run them on any execution engine.',\n",
    "        \"Beam's model is easy to use yet powerful in its capabilities.\"\n",
    "    ]\n",
    "\n",
    "    # Create a pipeline\n",
    "    with beam.Pipeline() as pipeline:\n",
    "        # Create a PCollection from the texts\n",
    "        pcollection = pipeline | 'Create PCollections' >> beam.Create(texts)\n",
    "\n",
    "        # Apply a transformation: split each text into words\n",
    "        words = pcollection | 'Split Words' >> beam.FlatMap(lambda text: text.split())\n",
    "\n",
    "        # Count the occurrences of each word\n",
    "        word_counts = words | 'Count Words' >> beam.combiners.Count.PerElement()\n",
    "\n",
    "        # Output the results to console\n",
    "        word_counts | 'Print Results' >> beam.Map(print)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf5372f1-4e3f-44e7-a3d3-ca99cf513ced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-deb1084e-e2c6-4580-8295-879af8918029.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The: 3\n",
      "Essence: 1\n",
      "of: 18\n",
      "Data: 15\n",
      "Engineering:: 1\n",
      "Building: 1\n",
      "the: 16\n",
      "Backbone: 1\n",
      "Modern: 1\n",
      "Infrastructure: 1\n",
      "In: 2\n",
      "today's: 1\n",
      "digital: 1\n",
      "age,: 1\n",
      "data: 31\n",
      "has: 1\n",
      "emerged: 1\n",
      "as: 7\n",
      "a: 8\n",
      "crucial: 2\n",
      "asset: 1\n",
      "for: 6\n",
      "organizations,: 1\n",
      "driving: 2\n",
      "decision-making: 1\n",
      "processes: 1\n",
      "and: 35\n",
      "enabling: 1\n",
      "innovation: 2\n",
      "across: 1\n",
      "industries.: 1\n",
      "At: 1\n",
      "heart: 1\n",
      "this: 2\n",
      "transformation: 1\n",
      "is: 12\n",
      "engineeringâ€”a: 1\n",
      "field: 1\n",
      "dedicated: 1\n",
      "to: 14\n",
      "designing,: 1\n",
      "constructing,: 1\n",
      "maintaining: 1\n",
      "infrastructure: 1\n",
      "required: 1\n",
      "manage: 1\n",
      "process: 2\n",
      "large: 1\n",
      "volumes: 1\n",
      "efficiently.: 1\n",
      "This: 3\n",
      "article: 1\n",
      "delves: 1\n",
      "into: 3\n",
      "core: 1\n",
      "aspects: 1\n",
      "engineering,: 2\n",
      "highlighting: 1\n",
      "its: 2\n",
      "importance,: 1\n",
      "key: 1\n",
      "components,: 1\n",
      "skills: 2\n",
      "necessary: 1\n",
      "excel: 2\n",
      "in: 7\n",
      "domain.: 1\n",
      "Importance: 1\n",
      "Engineering: 2\n",
      "engineering: 5\n",
      "forms: 1\n",
      "backbone: 1\n",
      "any: 1\n",
      "data-driven: 1\n",
      "organization.: 2\n",
      "It: 1\n",
      "ensures: 2\n",
      "that: 6\n",
      "collected,: 2\n",
      "stored,: 1\n",
      "processed,: 1\n",
      "made: 2\n",
      "accessible: 1\n",
      "manner: 1\n",
      "both: 1\n",
      "reliable: 1\n",
      "scalable.: 1\n",
      "Without: 1\n",
      "robust: 2\n",
      "practices,: 1\n",
      "organizations: 3\n",
      "would: 1\n",
      "struggle: 1\n",
      "harness: 1\n",
      "full: 2\n",
      "potential: 2\n",
      "their: 2\n",
      "data,: 4\n",
      "leading: 1\n",
      "missed: 1\n",
      "opportunities: 1\n",
      "inefficiencies.: 1\n",
      "Effective: 2\n",
      "enables: 1\n",
      "businesses: 1\n",
      "derive: 1\n",
      "actionable: 1\n",
      "insights,: 1\n",
      "improve: 1\n",
      "operational: 1\n",
      "efficiency,: 1\n",
      "maintain: 2\n",
      "competitive: 1\n",
      "edge.: 1\n",
      "Key: 2\n",
      "Components: 1\n",
      "1.: 1\n",
      "Collection: 1\n",
      "Ingestion:: 1\n",
      "first: 1\n",
      "step: 1\n",
      "involves: 3\n",
      "gathering: 1\n",
      "from: 2\n",
      "various: 2\n",
      "sources.: 1\n",
      "These: 2\n",
      "sources: 2\n",
      "can: 1\n",
      "be: 2\n",
      "internal: 1\n",
      "systems,: 1\n",
      "external: 1\n",
      "APIs,: 1\n",
      "IoT: 1\n",
      "devices,: 1\n",
      "or: 2\n",
      "third-party: 1\n",
      "services.: 1\n",
      "engineers: 6\n",
      "use: 2\n",
      "tools: 4\n",
      "like: 8\n",
      "Apache: 4\n",
      "Kafka,: 1\n",
      "AWS: 1\n",
      "Kinesis,: 1\n",
      "Flume: 1\n",
      "streamline: 1\n",
      "ingestion: 1\n",
      "process,: 1\n",
      "ensuring: 3\n",
      "flows: 1\n",
      "seamlessly: 1\n",
      "processing: 4\n",
      "pipeline.: 1\n",
      "2.: 1\n",
      "Storage:: 1\n",
      "Once: 1\n",
      "must: 2\n",
      "stored: 1\n",
      "way: 1\n",
      "supports: 1\n",
      "efficient: 3\n",
      "retrieval: 1\n",
      "analysis.: 2\n",
      "Depending: 1\n",
      "on: 2\n",
      "nature: 1\n",
      "might: 1\n",
      "relational: 1\n",
      "databases: 2\n",
      "PostgreSQL,: 1\n",
      "NoSQL: 1\n",
      "MongoDB,: 1\n",
      "warehouses: 1\n",
      "Amazon: 1\n",
      "Redshift: 1\n",
      "Google: 2\n",
      "BigQuery.: 1\n",
      "choice: 1\n",
      "storage: 1\n",
      "solution: 1\n",
      "critical: 1\n",
      "depends: 1\n",
      "factors: 1\n",
      "such: 3\n",
      "volume,: 1\n",
      "velocity,: 1\n",
      "variety,: 1\n",
      "specific: 1\n",
      "needs: 1\n",
      "3.: 1\n",
      "Processing:: 1\n",
      "design: 2\n",
      "implement: 2\n",
      "pipelines: 1\n",
      "raw: 1\n",
      "usable: 1\n",
      "format.: 1\n",
      "often: 2\n",
      "cleaning,: 1\n",
      "transforming,: 1\n",
      "aggregating: 1\n",
      "data.: 1\n",
      "Tools: 1\n",
      "Spark,: 2\n",
      "Hadoop,: 2\n",
      "Beam: 1\n",
      "are: 1\n",
      "commonly: 1\n",
      "used: 1\n",
      "handle: 1\n",
      "large-scale: 2\n",
      "tasks.: 2\n",
      "allow: 1\n",
      "batch: 1\n",
      "well: 1\n",
      "real-time: 1\n",
      "stream: 1\n",
      "processing,: 2\n",
      "catering: 1\n",
      "diverse: 1\n",
      "analytical: 3\n",
      "needs.: 1\n",
      "4.: 1\n",
      "Integration:: 1\n",
      "Ensuring: 1\n",
      "integrated: 1\n",
      "consistent: 1\n",
      "aspect: 1\n",
      "engineering.: 1\n",
      "working: 1\n",
      "with: 4\n",
      "ETL: 1\n",
      "(Extract,: 1\n",
      "Transform,: 1\n",
      "Load): 1\n",
      "Talend,: 1\n",
      "Informatica,: 1\n",
      "NiFi.: 1\n",
      "integration: 1\n",
      "remains: 1\n",
      "accurate,: 1\n",
      "up-to-date,: 1\n",
      "ready: 1\n",
      "5.: 1\n",
      "Security: 1\n",
      "Governance:: 1\n",
      "As: 1\n",
      "becomes: 1\n",
      "increasingly: 3\n",
      "valuable,: 1\n",
      "security: 3\n",
      "compliance: 2\n",
      "regulations: 1\n",
      "paramount.: 1\n",
      "measures,: 1\n",
      "including: 1\n",
      "encryption,: 1\n",
      "access: 1\n",
      "controls,: 1\n",
      "auditing: 1\n",
      "mechanisms.: 1\n",
      "They: 1\n",
      "also: 1\n",
      "establish: 1\n",
      "governance: 1\n",
      "practices: 1\n",
      "quality,: 1\n",
      "lineage,: 1\n",
      "standards: 1\n",
      "GDPR: 1\n",
      "CCPA.: 1\n",
      "Essential: 1\n",
      "Skills: 1\n",
      "Engineers: 1\n",
      "To: 1\n",
      "professionals: 1\n",
      "possess: 1\n",
      "blend: 1\n",
      "technical: 1\n",
      "skills.: 1\n",
      "competencies: 1\n",
      "include:: 1\n",
      "-: 5\n",
      "Programming:: 1\n",
      "Proficiency: 1\n",
      "languages: 1\n",
      "Python,: 1\n",
      "Java,: 1\n",
      "SQL: 1\n",
      "essential: 1\n",
      "building: 1\n",
      "managing: 1\n",
      "pipelines.: 1\n",
      "Database: 1\n",
      "Management:: 1\n",
      "Understanding: 1\n",
      "different: 1\n",
      "database: 1\n",
      "architectures: 1\n",
      "query: 1\n",
      "optimization: 1\n",
      "techniques: 1\n",
      "crucial.: 1\n",
      "Big: 1\n",
      "Technologies:: 1\n",
      "Familiarity: 1\n",
      "Kafka: 1\n",
      "vital: 1\n",
      "handling: 1\n",
      "Cloud: 1\n",
      "Platforms:: 1\n",
      "Knowledge: 1\n",
      "cloud: 1\n",
      "services: 1\n",
      "AWS,: 1\n",
      "Cloud,: 1\n",
      "Azure: 1\n",
      "important: 1\n",
      "migrate: 1\n",
      "cloud.: 1\n",
      "Problem-Solving:: 1\n",
      "Strong: 1\n",
      "problem-solving: 1\n",
      "enable: 1\n",
      "solutions: 1\n",
      "troubleshoot: 1\n",
      "issues: 1\n",
      "effectively.: 1\n",
      "conclusion,: 1\n",
      "foundational: 1\n",
      "element: 1\n",
      "modern: 1\n",
      "ecosystems.: 1\n",
      "By: 1\n",
      "collection,: 1\n",
      "storage,: 1\n",
      "empower: 1\n",
      "unlock: 1\n",
      "growth: 1\n",
      "an: 1\n",
      "data-centric: 1\n",
      "world.: 1\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions\n",
    "\n",
    "def run(argv=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input',\n",
    "                        dest='input',\n",
    "                        default='gs://sandeep-apache/data.txt',\n",
    "                        help='Input file path in GCS.')\n",
    "    parser.add_argument('--output',\n",
    "                        dest='output',\n",
    "                        default='gs://sandeep-apache/output.txt',\n",
    "                        help='Output file path in GCS.')\n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "    \n",
    "    # Define PipelineOptions with Google Cloud specific options\n",
    "    options = PipelineOptions(pipeline_args)\n",
    "    google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "    google_cloud_options.region = 'us-east-1'  # Replace with your preferred GCP region\n",
    "    google_cloud_options.temp_location = 'gs://sandeep-apache/temp'  # Temporary storage location\n",
    "    google_cloud_options.staging_location = 'gs://sandeep-apache/staging'  # Staging location for dataflow jobs\n",
    "    \n",
    "    with beam.Pipeline(options=options) as pipeline:\n",
    "        # Read from the specified input file in GCS\n",
    "        lines = pipeline | 'ReadFromText' >> beam.io.ReadFromText(known_args.input)\n",
    "        \n",
    "        # Count the occurrences of each word\n",
    "        word_counts = (\n",
    "            lines\n",
    "            | 'Split Words' >> beam.FlatMap(lambda line: line.split())\n",
    "            | 'Count Words' >> beam.combiners.Count.PerElement()\n",
    "        )\n",
    "        \n",
    "        # Format the output and print to console\n",
    "        def format_output(word_count):\n",
    "            (word, count) = word_count\n",
    "            return f'{word}: {count}'\n",
    "        \n",
    "        word_counts | 'FormatOutput' >> beam.Map(format_output) | 'PrintResults' >> beam.Map(print)\n",
    "        \n",
    "        # Write the output to the specified output file in GCS\n",
    "        # Commenting out this line since we are printing instead of writing to a file\n",
    "        # word_counts | 'WriteToText' >> beam.io.WriteToText(known_args.output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3c5ed89-7bb3-4745-80d1-b000e97dc5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-deb1084e-e2c6-4580-8295-879af8918029.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Anil Kumar', 'surname': 'Yellamandala', 'age': 21}\n",
      "{'name': 'Sandipan', 'surname': 'Pramanik', 'age': 21}\n",
      "{'name': 'Pankaj', 'surname': 'Sharma', 'age': 21}\n",
      "{'name': 'Subodh', 'surname': 'Agrawal', 'age': 21}\n",
      "{'name': 'Sweta', 'surname': 'Ghatak', 'age': 23}\n",
      "{'name': 'Himanshu', 'surname': 'Gupta', 'age': 23}\n",
      "{'name': 'Papia', 'surname': 'Dastidar', 'age': 23}\n",
      "{'name': 'Rohit', 'surname': 'Manhas', 'age': 26}\n",
      "{'name': 'AMIT', 'surname': 'CHOUDHARY', 'age': 26}\n",
      "{'name': 'Shikha', 'surname': 'Jain', 'age': 26}\n",
      "{'name': 'MAHESH', 'surname': 'SUBBAIAH', 'age': 26}\n",
      "{'name': 'Shambhu', 'surname': 'Sharma', 'age': 27}\n",
      "{'name': 'Ritam', 'surname': 'Bit', 'age': 27}\n",
      "{'name': 'Krishna', 'surname': 'Choudhury', 'age': 27}\n",
      "{'name': 'Abhishek', 'surname': 'Dubey', 'age': 27}\n",
      "{'name': 'GAUTAM', 'surname': 'DUTTA', 'age': 33}\n",
      "{'name': 'Pankaj', 'surname': 'Tiwary', 'age': 33}\n",
      "{'name': 'K S', 'surname': 'Suraj', 'age': 33}\n",
      "{'name': 'Virgina', 'surname': 'Priyadarshini D S', 'age': 33}\n",
      "{'name': 'Jayasree', 'surname': 'ghosh', 'age': 33}\n",
      "{'name': 'Kalaivani', 'surname': 'G S', 'age': 34}\n",
      "{'name': 'Santosh', 'surname': 'Kumar', 'age': 43}\n",
      "{'name': 'IBRAHIM HANIF', 'surname': 'AKBAR', 'age': 45}\n",
      "{'name': 'Raghavendra', 'surname': 'Vishwanath', 'age': 45}\n",
      "{'name': 'Virendra', 'surname': 'Kumar', 'age': 45}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions\n",
    "\n",
    "def run(argv=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--output',\n",
    "                        dest='output',\n",
    "                        default='gs://sandeep-apache/output.txt',\n",
    "                        help='Output file path in GCS.')\n",
    "    parser.add_argument('--project',\n",
    "                        dest='project',\n",
    "                        default='techlanders-internal',\n",
    "                        help='GCP project ID.')\n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "    \n",
    "    # Define the default BigQuery table\n",
    "    default_table = 'notebook_dataset.users'  # Replace with your default BigQuery table\n",
    "    \n",
    "    # Define PipelineOptions with Google Cloud specific options\n",
    "    options = PipelineOptions(pipeline_args)\n",
    "    google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "    google_cloud_options.project = known_args.project\n",
    "    google_cloud_options.region = 'us-east-1'  # Replace with your preferred GCP region\n",
    "    google_cloud_options.temp_location = 'gs://sandeep-apache/temp'  # Temporary storage location\n",
    "    google_cloud_options.staging_location = 'gs://sandeep-apache/staging'  # Staging location for Dataflow jobs\n",
    "    \n",
    "    with beam.Pipeline(options=options) as pipeline:\n",
    "        # Read from BigQuery\n",
    "        query = f'SELECT name, surname, age FROM `{default_table}`'\n",
    "        rows = pipeline | 'ReadFromBigQuery' >> beam.io.ReadFromBigQuery(\n",
    "            query=query,\n",
    "            use_standard_sql=True\n",
    "        )\n",
    "        \n",
    "        # Print the rows to console\n",
    "        rows | 'PrintResults' >> beam.Map(print)\n",
    "        \n",
    "        # Write the output to the specified output file in GCS\n",
    "        # Commenting out this line since we are printing instead of writing to a file\n",
    "        # rows | 'WriteToText' >> beam.io.WriteToText(known_args.output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e11f32c-30a5-443a-b285-4a5a804b9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions\n",
    "\n",
    "def run(argv=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--output',\n",
    "                        dest='output',\n",
    "                        default='gs://sandeep-apache/data.txt',\n",
    "                        help='Output file path in GCS.')\n",
    "    parser.add_argument('--project',\n",
    "                        dest='project',\n",
    "                        default='techlanders-internal',\n",
    "                        help='GCP project ID.')\n",
    "    parser.add_argument('--dataset',\n",
    "                        dest='dataset',\n",
    "                        default='notebook_dataset',\n",
    "                        help='BigQuery dataset name.')\n",
    "    parser.add_argument('--table',\n",
    "                        dest='table',\n",
    "                        default='users',\n",
    "                        help='BigQuery table name.')\n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "    \n",
    "    # Define PipelineOptions with Google Cloud specific options\n",
    "    options = PipelineOptions(pipeline_args)\n",
    "    google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "    google_cloud_options.project = known_args.project\n",
    "    google_cloud_options.region = 'us-east-1'  # Replace with your preferred GCP region\n",
    "    google_cloud_options.temp_location = 'gs://sandeep-apache/temp'  # Temporary storage location\n",
    "    google_cloud_options.staging_location = 'gs://sandeep-apache/staging'  # Staging location for Dataflow jobs\n",
    "    \n",
    "    with beam.Pipeline(options=options) as pipeline:\n",
    "        # Define the table_spec\n",
    "        table_spec = f'{known_args.project}:{known_args.dataset}.{known_args.table}'\n",
    "        \n",
    "        # Read from BigQuery\n",
    "        query = f'SELECT * FROM `{table_spec}`'\n",
    "        rows = pipeline | 'ReadFromBigQuery' >> beam.io.ReadFromBigQuery(\n",
    "            query=query,\n",
    "            use_standard_sql=True\n",
    "        )\n",
    "        \n",
    "        # Print the rows to console\n",
    "        rows | 'PrintResults' >> beam.Map(print)\n",
    "        \n",
    "        # Write the output to the specified output file in GCS\n",
    "        # Commenting out this line since we are printing instead of writing to a file\n",
    "        # rows | 'WriteToText' >> beam.io.WriteToText(known_args.output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "apache-beam-2.54.0",
   "name": ".m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m121"
  },
  "kernelspec": {
   "display_name": "Apache Beam 2.54.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.54.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
